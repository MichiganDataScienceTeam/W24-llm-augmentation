{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll go over word and sentence embeddings. We'll use the pre-trained GloVe embeddings model to get the embeddings for different phrases, and compare the embeddings for different sentences and visualize them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from scipy import spatial\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to write a function to get the embedding vector for a sentence. We can use the model to get the embedding vector for each word, or token, in the sentence. For now, let's obtain the sentence embedding by naively taking the average of each word embedding in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that gets the embedding vector for a sentence/phrase by taking the average of the word embedding vectors\n",
    "def get_embedding(sentence, model=model):\n",
    "    # TO-DO\n",
    "    # Split the sentence into words, and apply any other preprocessing steps you may want\n",
    "    # Get the embedding vector for each word\n",
    "    # Return the average of the vectors (use np.mean)\n",
    "    # Tokenize the sentence\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are just vectors in an embedding space, where the distance between the vectors reflect semantic similarity. If we write a function to calculate the distance between two vectors, we can evaluate the similarity between two sentences using their embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common ways to find the distance between two vectors include Euclidean Distance, Cosine Similarity, and the Dot Product. Pick any one of these three and implement them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(sentence1, sentence2, model=model):\n",
    "    # TO-DO\n",
    "    # Get the embedding vector for each sentence\n",
    "    # Calculate distance between the two vectors\n",
    "    # scipy has functions that can help (e.g. scipy.spatial.distance.cosine, scipy.spatial.distance.euclidean, etc.)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example words\n",
    "sentence1 = \"I took my dog to the park\"\n",
    "sentence2 = \"cats in nature\"\n",
    "\n",
    "# Calculate distance between embeddings\n",
    "distance = calculate_distance(sentence1, sentence2, model)\n",
    "print(f\"The distance between '{sentence1}' and '{sentence2}' is: {distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I took my dog to the park\"\n",
    "sentence2 = \"Cats in nature\"\n",
    "\n",
    "embedding1 = get_embedding(sentence1, model)\n",
    "embedding2 = get_embedding(sentence2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dimension of embedding: {embedding1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings are very high dimension vectors. To be able to visualize them, we want to reduce their dimensionality. One common way to do this is [principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis), or PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings_with_similarity(sentences, model):\n",
    "    embeddings = np.array([get_embedding(sentence, model) for sentence in sentences])\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1])\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        plt.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], sentence)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.title('Sentence Embeddings Visualization with PCA')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "sentences = [\"I took my dog to the park\", \n",
    "             \"Cats in nature\", \n",
    "             \"I love ice cream\", \n",
    "             \"Learning japanese sucks\",\n",
    "             \"naval warfare in the pacific\"\n",
    "            ]\n",
    "visualize_embeddings_with_similarity(sentences, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visual we've generated makes sense, but the lack of dimensions leaves much to be desired. Let's try this again, but with 3 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings_with_similarity_3d(sentences, model):\n",
    "    embeddings = np.array([get_embedding(sentence, model) for sentence in sentences])\n",
    "\n",
    "    # Apply PCA with 3 components\n",
    "    pca = PCA(n_components=3)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], reduced_embeddings[:, 2])\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        ax.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], reduced_embeddings[i, 2], sentence)\n",
    "    ax.set_xlabel('PCA Component 1')\n",
    "    ax.set_ylabel('PCA Component 2')\n",
    "    ax.set_zlabel('PCA Component 3')\n",
    "    plt.title('Sentence Embeddings Visualization with PCA (3D)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings_with_similarity_3d(sentences, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization of the embedding vectors in 3 dimensions more accurately distinguishes the meaning of the sentences from one another. The more dimensions we have, the more accurately and precisely we can discriminate between the meaning of different sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to challenge yourself, implement the dimensionality reduction again, this time using [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding), another common dimensionality reduction method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
